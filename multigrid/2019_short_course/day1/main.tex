\documentclass[18pt,xcolor=table]{beamer}

\input{./pres_style.tex}

%%%%%%%%%%%%%%%%%%%%%%%
% user-defined commands
%%%%%%%%%%%%%%%%%%%%%%%
\input{./macros.tex}%added macro definitions here

\usepackage{tikz}
\usepackage{tabularx}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{arrows,positioning} 

\usepackage{cancel}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[]{algorithm}
\usepackage{algpseudocode}
\captionsetup{compatibility=false}


\title[Multigrid]{Introduction to Multigrid Methods}
\subtitle{Day 1: Motivation and Basic Iterative Methods}
\author[Mitchell]{Wayne Mitchell}
\institute{\pgfuseimage{logo}\\Universit\"at Heidelberg\\Institut f\"ur Technische Informatik}
\date[]{\alert{}}


\begin{document}
\input{./slide_style.tex}

\DeclareRobustCommand{\Chi}{\raisebox{2pt}{$\chi$}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{\bf Outline:}
\framesubtitle{~~}
\tableofcontents
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Course introduction}

% Slide
\begin{frame}{Course introduction}
\begin{block}{Goals}
\bit
\item Basics of scientific computing and motivations for multigrid
\item Understanding of basic multigrid principles
\item Overview of various multigrid algorithms
\item Mention some current research topics
\eit
\end{block}
\begin{block}{Acknowledgements}
\bit
\item These slides are based on previous tutorials by Steve McCormick, Van Henson, Rob Falgout, Irad Yavneh, David Moulton.
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Course introduction}
\begin{block}{Resources}
\bit
\item 
\eit
\end{block}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Motivation}

% Slide
\begin{frame}{Motivation}
\begin{block}{In this section}
\bit
\item General scientific computing a simulation and where large sparse linear systems come from
\item Model Poisson problem
\item Properties of matrix coming from Poisson (i.e. SPD... anything else?) and assumptions (for now) on the matrices we'll be using in general
\item Methods for solving linear systems: direct vs. iterative, optimal complexity, the need for efficient solvers
\eit
\end{block}
\end{frame}

\begin{frame}{Motivation}
\begin{block}{Scientific computing}
\bit
\item The three pillars of science: theory, experiments, and \emph{simulation}
\item Computation can allow study of physical phenomena that are difficult, expensive, or infeasible to study with experiments
\item More computational power and more sophisticated algorithms enable more complicated simulations with better accuracy
\eit
\end{block}
\end{frame}

\begin{frame}{Motivation}
\begin{block}{Simulation pipeline}
\bit
\item Physical phenomenon (potential field)
\item Continuous equations (Poisson's equation)
\item Discrete problem formulation (finite difference method)
\item Form discrete system of equations (meshing)
\only<1>{\item Solve the discrete system (linear/non-linear solvers)}
\only<2>{\item \textbf{Solve the discrete system (linear/non-linear solvers)}}
\eit
\end{block}
\end{frame}

\begin{frame}{Motivation}
\begin{block}{Large, sparse linear systems}
\bit
\item Resulting linear systems tend to be large and sparse
\item Size (and conditioning) of the system depends on mesh resolution
\item Solving these linear systems is often a computational bottleneck
\item Want solvers to be:
\bit
\item Efficient (as problem grows, execution time does not suffer)
\item Robust (as conditioning gets worse, convergence does not suffer)
\eit
\eit
\end{block}
\end{frame}

\begin{frame}{Motivation}
\begin{block}{Direct methods}
\bit
\item General purpose direct methods:
\bit
\item Matrix factorizations: Cholesky or LU (Gaussian elimination), QR, SVD 
\item Not generally scalable: $O(N^3)$ computational cost
\eit
\item Problem specific direct methods:
\bit
\item Fast Fourier transfrom, cyclic reduction, fast multipole
\item More scalabe: $O(N\log N)$ or even $O(N)$ computational cost
\eit
\eit
\end{block}
\end{frame}

\begin{frame}{Motivation}
\begin{block}{Iterative methods}
\bit
\item Basic iterative methods:
\bit
\item Jacobi, Gauss-Seidel
\item Cheap to apply, $O(N)$, but generally poor convergence
\eit
\item Krylov methods:
\bit
\item Conjugate gradient, GMRES, BiCGSTAB
\item Also $O(N)$ with much better convergence, but usually requires preconditioning
\eit
\item Multilevel methods:
\bit
\item $O(N)$ solvers and preconditioners with robust convergence 
\eit
\eit
\end{block}
\end{frame}

% BUT iterative methods don't give you the exact solution, right? Isn't that a problem? No. Tunable accuracy is actually preferred due to discretization accuracy, etc.
\begin{frame}{Motivation}
\begin{block}{Numerical accuracy}
\bit
\item Discretization accuracy
\item Accuracy of the algebraic solve
\eit
\end{block}
\end{frame}

\begin{frame}{Motivation}
\begin{block}{The slide where you show how long it takes to solve the same problem with a bunch of different methods of increasing efficiency}
\bit
\item 
\eit
\end{block}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Basic iterative methods}

\begin{frame}{Basic iterative methods}
\begin{block}{In this section}
\bit
\item Matrix splittings, fixed point methods
\item Basic convergence of splitting methods
\item Jacobi and Gauss-Seidel
\item Application of convergence theory to Jacobi
\item Drawbacks: n dependent convergence that gets very slow as n gets large
\eit
\end{block}
\end{frame}


\begin{frame}{Basic iterative methods}
\begin{block}{Iterative methods}
\bit
\item Want to solve $A\mathbf{u} = \mathbf{f}$
\item Start with initial guess, $\mathbf{u}^{(0)}$
\item Apply iterative process, $\mathbf{u}^{(i+1)} = S(\mathbf{u}^{(i)})$, $i =0,1,...$
\item Hope that $||\mathbf{u} - \mathbf{u}^{(i)}||$ is acceptably small for reasonable $i$
\eit
\end{block}
\end{frame}


\begin{frame}{Basic iterative methods}
\begin{block}{Jacobi iteration}
\bit
\item Simple, point-wise scheme
\item Solve each equation in the system independently, solving the $i^{th}$ equation for the $i^{th}$ unknown
\eit
\eq{
\begin{bmatrix}
a_{0,0} & a_{0,1} & a_{0,2} &... & & & & \\
a_{1,0} & a_{1,1} & a_{1,2} &... & & & & \\
a_{2,0} & a_{2,1} & a_{2,2} & & & & & \\
\ddots&  & \ddots & \ddots & \ddots & & & \\
& & & & & & & \\
& & & & & & & \\
& & & & & & & a_{N-1,N-1} \\
\end{bmatrix}
\begin{bmatrix}
u_0 \\
u_2 \\
\\
\vdots \\
\\
\\
u_{N-1} \\
\end{bmatrix}
=
\begin{bmatrix}
f_0 \\
f_2 \\
\\
\vdots \\
\\
\\
f_{N-1} \\
\end{bmatrix}
}
\end{block}
\end{frame}

\begin{frame}{Basic iterative methods}
\begin{block}{Jacobi iteration}
\bit
\item The $i^{th}$ equation is
\eit
\eq{
\sum_j a_{i,j}u_j &= f_i \\
a_{i,i}u_i + \sum_{j\neq i} a_{i,j}u_j &= f_i \\
u_i &= \frac{1}{a_{i,i}} (f_i - \sum_{j\neq i} a_{i,j}u_j)
}
\bit
\item One iteration of Jacobi: for all $i$, set $u^{(k+1)}_i = \frac{1}{a_{i,i}} (f_i - \sum_{j\neq i} a_{i,j}u^{(k)}_j)$
\eit
\end{block}
\end{frame}

\begin{frame}{Basic iterative methods}
\begin{block}{Gauss-Seidel}
\bit
\item Gauss-Seidel solves the equations one after another
\item Similar to Jacobi, but uses updated solution
\item One iteration of Jacobi: for $i = 0,1,...$, overwrite $u_i \leftarrow \frac{1}{a_{i,i}} (f_i - \sum_{j\neq i} a_{i,j}u_j)$
\eit
\end{block}
\end{frame}

\begin{frame}{Basic iterative methods}
\begin{block}{Matrix representations of iterative methods}
\bit
\item Matrix form of Jacobi:
\eit
\eq{
A\mathbf{u} &= \mathbf{f} \\
(D - L - U)\mathbf{u} &= \mathbf{f} \\
D\mathbf{u} &= \mathbf{f} + (L + U)\mathbf{u} \\
\mathbf{u} &= D^{-1}(\mathbf{f} + (L+U)\mathbf{u})
}
\bit
\item One iteration of Jacobi: $\mathbf{u} \leftarrow D^{-1}(\mathbf{f} + (L+U)\mathbf{u})$
\eit
\end{block}
\end{frame}

\begin{frame}{Basic iterative methods}
\begin{block}{Matrix representations of iterative methods}
\bit
\item Matrix form of Gauss-Seidel:
\eit
\eq{
A\mathbf{u} &= \mathbf{f} \\
(D - L - U)\mathbf{u} &= \mathbf{f} \\
(D - L)\mathbf{u} &= \mathbf{f} + U\mathbf{u} \\
\mathbf{u} &= (D-L)^{-1}(\mathbf{f} + U\mathbf{u})
}
\bit
\item One iteration of Gauss-Seidel: $\mathbf{u} \leftarrow (D-L)^{-1}(\mathbf{f} + U\mathbf{u})$
\eit
\end{block}
\end{frame}

\begin{frame}{Basic iterative methods}
\begin{block}{General matrix splittings}
\bit
\item Jacobi and Gauss-Seidel are both examples of matrix splittings
\item General matrix splitting: $A = M - N$
\item Iteration: $\mathbf{u} \leftarrow M^{-1}(\mathbf{f} + N\mathbf{u})$
\eit
\end{block}
\end{frame}

\begin{frame}{Basic iterative methods}
\begin{block}{Matrix splitting error analysis}
\bit
\item Let $A\mathbf{u} = \mathbf{f}$, $\mathbf{u}^{(k)}$, the $k^{th}$ iterate, and $\mathbf{e}^{(k)} = \mathbf{u}^{(k)} - \mathbf{u}$
\eit
\eq{
M\mathbf{u}^{(k)} &= \mathbf{f} + N\mathbf{u}^{(k-1)} \\
M\mathbf{u} &= \mathbf{f} + N\mathbf{u} \\
\Rightarrow M\mathbf{e}^{(k)} &= N\mathbf{e}^{(k-1)} \\
\mathbf{e}^{(k)} &= M^{-1}N\mathbf{e}^{(k-1)} \\
\mathbf{e}^{(k)} &= (M^{-1}N)^k\mathbf{e}^{(0)}
}
\bit
\item The iteration converges if and only if $(M^{-1}N)^k \rightarrow 0$ as $k\rightarrow \infty$, or equivalently $\rho (M^{-1}N) < 1$
\eit
\end{block}
\end{frame}

\begin{frame}{Basic iterative methods}
\begin{block}{Convergence of Jacobi and Gauss-Seidel}
\bit
\item If $A$ is strictly diagonally dominant, then Jacobi converges
\item If $A$ is symmetric positive definite (SPD), then Gauss-Seidel converges
\item Speed of convergence depends on $\rho (M^{-1}N)$ and can be \emph{very} slow
\eit
\end{block}
\end{frame}

\begin{frame}{Basic iterative methods}
\begin{block}{}
\bit
\item 
\eit
\end{block}
\end{frame}

\begin{frame}{Basic iterative methods}
\begin{block}{}
\bit
\item 
\eit
\end{block}
\end{frame}


% Basic convergence theory for splitting methods and apply to Jacobi and GS
% Something on motivating weighted Jacobi. Show how weight affects which modes are damped and by how much for Jacobi. Show curves.
% Show performance of different methods
% Discuss idea of red-black GS as a way to get more parallelism







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Model problem}

\begin{frame}{Model problem}
\begin{block}{Model problem: Poisson}
\bit
\item Classic model problem is a simple Poisson problem with zero Dirichlet boundary conditions:
\eq{
\Delta u &= f, & \Omega \\
u &= 0, & \partial \Omega
}
\item Discretizing with finite differences on a regular 1D mesh:
\eit
\eq{
&\frac{-u_{i-1} + 2u_i - u_{i+1}}{h^2} = f_i, & i = 1,2,..., N\\
&u_0 = u_{N+1} = 0
}
\end{block}
\end{frame}

\begin{frame}{Model problem}
\begin{block}{Model problem: Poisson}
\begin{center}
\includegraphics[width=0.7\textwidth]{../figures/1DFDPoisson}
\end{center}
\eq{
\frac{1}{h^2}\begin{bmatrix}
2 & -1 & & & & & & \\
-1 & 2 & -1 & & & & & \\
& -1 & 2 & - 1 & & & & \\
&  & \ddots & \ddots & \ddots & & & \\
& & & & & & & \\
& & & & & -1 & 2 & -1 \\
& & & & & & -1 & 2 \\
\end{bmatrix}
\begin{bmatrix}
u_1 \\
u_2 \\
\\
\vdots \\
\\
\\
u_N \\
\end{bmatrix}
=
\begin{bmatrix}
f_1 \\
f_2 \\
\\
\vdots \\
\\
\\
f_N \\
\end{bmatrix}
}
\end{block}
\end{frame}

% TODO: Make visualizations (jupyter notebook?) that shows Jacobi and GS solves step by step for 1D poisson problem
% TODO: discuss condition number
% TODO: general matrix preliminaries: SPD, diagonal dominant definitions, maybe some properties, condition number


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Krylov subspace methods}

\begin{frame}{Krylov subspace methods}
\begin{block}{In this section}
\bit
\item Gradient descent
\item Conjugate gradient
\item CG converges in $n$ iterations, but usually far less
\item Mention GMRES and BiCGSTAB for non-symmetric
\eit
\end{block}
\end{frame}

\begin{frame}{Krylov subspace methods}
\begin{block}{Definition of a Krylov subspace}
\bit
\item $\mathcal{K}_r(A,\mathbf{f}) = \text{span}\{\mathbf{f},A\mathbf{f},A^2\mathbf{f},...,A^{r-1}\mathbf{f}\}$
\item TODO: why might this be a good space to search in?
\eit
\end{block}
\end{frame}

\begin{frame}{Krylov subspace methods}
\begin{block}{Gradient descent}
\bit
\item Assume $A$ is SPD
\item Recast $A\mathbf{u} = \mathbf{f}$ as a minimization problem:
\eq{
   \phi(\mathbf{u}) = \frac{1}{2}\mathbf{u}^TA\mathbf{u} - \mathbf{u}^T\mathbf{f}
}
\item Note that $-\nabla \phi(\mathbf{u}) = \mathbf{f} - A\mathbf{u}$
\item Thus $\phi(\mathbf{u})$ is minimized at the solution $A^{-1}\mathbf{f}$ and the direction of \emph{steepest descent} towards that minimum is in the direction of the residual, $\mathbf{r} = \mathbf{f} = A\mathbf{u}$
\eit
\end{block}
\end{frame}

% TODO: visualization of gradient descent

\begin{frame}{Krylov subspace methods}
\begin{block}{Gradient descent}
\bit
\item $\mathbf{r}$ definest the search direction, and $\alpha = \langle \mathbf{r}, \mathbf{r} \rangle / \langle \mathbf{r}, A\mathbf{r} \rangle$
\item $\alpha$ is chosen by an exact line search to minimize $\phi(\mathbf{u} + \alpha \mathbf{r})$
\eit
\end{block}
\begin{algorithm}[H]
\caption{Gradient descent}
\begin{algorithmic}
\State Set $\mathbf{u}$ initial guess
\For{k = 0,1,...}
\State $\mathbf{r} \leftarrow \mathbf{f} - A\mathbf{u}$
\State $\alpha \leftarrow \langle \mathbf{r}, \mathbf{r} \rangle / \langle \mathbf{r}, A\mathbf{r} \rangle$
\State $\mathbf{u} \leftarrow \mathbf{u} + \alpha\mathbf{r}$
\EndFor
\end{algorithmic}
\end{algorithm}
\end{frame}

\begin{frame}{Krylov subspace methods}
\begin{block}{Gradient descent}
\bit
\item TODO: visualize gradient descent
\eit
\end{block}
\end{frame}

\begin{frame}{Krylov subspace methods}
\begin{block}{Gradient descent}
\bit
\item TODO: visualize canyon pitfall of gradient descent
\eit
\end{block}
\end{frame}

\begin{frame}{Krylov subspace methods}
\begin{block}{Conjugate Gradient}
\bit
\item 
\eit
\end{block}
\end{frame}

\begin{frame}{Krylov subspace methods}
\begin{block}{}
\bit
\item 
\eit
\end{block}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Preconditiong Krylov methods}

\begin{frame}{Preconditiong Krylov methods}
\begin{block}{In this section}
\bit
\item Why does ill conditioning hurt CG?
\item How to do preconditioning
\item Couple examples of preconditioners: diagonal
\eit
\end{block}
\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

