\documentclass[18pt,xcolor=table]{beamer}

\input{./pres_style.tex}

%%%%%%%%%%%%%%%%%%%%%%%
% user-defined commands
%%%%%%%%%%%%%%%%%%%%%%%
\input{./macros.tex}%added macro definitions here

\usepackage{tikz}
\usepackage{tabularx}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{arrows,positioning} 

\usepackage{cancel}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[]{algorithm}
\usepackage{algpseudocode}
\captionsetup{compatibility=false}


\title[Multigrid]{Introduction to Multigrid Methods}
\subtitle{Day 5: parallel AMG}
\author[Mitchell]{Wayne Mitchell}
\institute{\pgfuseimage{logo}\\Universit\"at Heidelberg\\Institut f\"ur Technische Informatik}
\date[]{\alert{}}


\begin{document}
\input{./slide_style.tex}

\DeclareRobustCommand{\Chi}{\raisebox{2pt}{$\chi$}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Slide
\begin{frame}{}
\begin{block}{Day 5 Goals}
\bit
\item Session 1:
\bit
\item Development of \emph{hypre} BoomerAMG
\eit
\item Session 2:
\bit
\item Algebraic multigrid domain decomposition (AMG-DD)
\eit
\item Session 3:
\bit
\item Discussion and hands-on examples
\eit
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}
\frametitle{\bf Outline:}
\framesubtitle{~~}
\tableofcontents
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Development of \emph{hypre}}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{Parallel AMG}
\bit
\item Recall that the appeal of multigrid is its $O(N)$ computational cost
\item Want to use multigrid for \emph{very} large problems on large parallel machines
\item Efficient parallelization of multigrid, particularly AMG, is non-trivial
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{Parallel AMG}
\bit
\item AMG setup: need parallel coarsening algorithms and corresponding interpolation strategies
\item AMG solve: need parallel smoothers
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{AMG coarsening in parallel}
\bit
\item Henson and Yang. BoomerAMG: A parallel algebraic multigrid solver and preconditioner. Appl. Num. Math. 2002.
\item Classical coloring algorithm is sequential in nature
\item First attempts at parallel coarsening:
\bit
\item Cleary-Luby-Jones-Plassman (CLJP) coarsening
\item Falgout coarsening
\eit
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{CLJP coarsening}
\bit
\item Based on parallel graph partitioning
\item Entirely parallel, same coarse grids independent of number of processors
\item Admits fine-grained parallelism
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{CLJP coarsening}
\bit
\item Recall strength of connection (SoC): $i$ strongly depends on $j$ if $-a_{i,j} \geq \theta\max_{k\neq i}(-a_{i,k})$
\item Define the SoC matrix, $S$, by $s_{i,j} = 1$ if $i$ strongly depends on $j$
\item Rows, $S_i$, of $S$ define the set of points that strongly influence $i$
\item Columns, $S_i^T$, of $S$ define the set of points that $i$ strongly influences
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{CLJP coarsening}
\bit
\item Define a measure $w(i) = |S_i^T| + \sigma(i)$, where $\sigma(i)$ is a random number in $(0,1)$
\item Select a set $D$, where $i\in D$ if $w(i)>w(j)$ for all $j\in S_i\cup S_i^T$
\item Note that $D$ is an independent set that can be selected in parallel
\item Consider the following heuristics:
\bit
\item C-points are not interpolated from neighbors, so neighbors are less valuable as potential C-points
\item If C-point $i$ influences points $j$ and $k$, and $j$ influences $k$, then $j$ is less valuable as a C-point
\eit
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{algorithm}[H]
\caption{CLJP coarsening}
\begin{algorithmic}
\State Select $D = \{i : w(i)>w(j) , \forall j\in S_i\cup S_i^T\}$
\While{ $D\neq \emptyset$ }
\State Mark points in $D$ as C-points
\For{$i\in D$}
   \State For each $j\in S_i$, decrement $w(j)$ and remove edge $S_{i,j}$
   \For{$j\in S_i^T$}
      \State Remove edge $S_{j,i}$
      \For{$k\in S_j^T : S_{k,i}\neq 0$}
         \State Decrement $w(j)$ and remove edge $S_{k,j}$
      \EndFor
   \EndFor
\EndFor
\State Select new $D$ (communicate weights at processor boundaries)
\EndWhile
\end{algorithmic}
\end{algorithm}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\only<1>{
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/cljpCoarsening1}
\end{center}}
\only<2>{
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/cljpCoarsening2}
\end{center}}
\only<3>{
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/cljpCoarsening3}
\end{center}}
\only<4>{
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/cljpCoarsening4}
\end{center}}
\only<5>{
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/cljpCoarsening5}
\end{center}}
\only<6>{
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/cljpCoarsening6}
\end{center}}
\only<7>{
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/cljpCoarsening7}
\end{center}}
\tiny{Slide credit: Rob Falgout, Copper Mountain 2019 Parallel Multigrid Tutorial}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{CLJP coarsening}
\bit
\item Pros and cons of CLJP coarsening:
\bit
\item Entirely parallel
\item No issues proceeding to arbitrarily coarse grids (some processor drop out)
\item Tends to produce more C-points than classical coarsening 
\item Higher computation and communication complexity on coarse grids
\eit
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{Falgout coarsening}
\bit
\item A more natural extension of classical AMG coarsening in parallel is simply to allow each processor to independently run the Ruge-St\"uben coloring algorithm
\item This yields conflicts at processor boundaries that need to be resolved
\item Several strategies, for this, but the most successful method is to combine Ruge-St\"uben coarsening and CLJP coarsening
\item Result is Falgout coarsening
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{Falgout coarsening}
\bit
\item Start by running both passes of Ruge-St\"uben coloring algorithm independently on each processor
\item User resulting \emph{interior} C-points as initial set, $D$
\item Run CLJP to finish assigning boundary points
\item Results in much nicer coarsenings on processor interiors 
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{PMIS coarsening}
\bit
\item Especially for 3D problems coarse-grid complexity is an issue for Falgout coarsening
\item Recall the heuristics used for classical AMG coarsening:
\begin{enumerate}
\item For each F-point, $i$, every point $j\in S_i$ is either in $C_i$ or strongly depends on at least one point in $C_i$
\item The set of C-points is a maximal subset such that no C-point strongly depends on another C-point.
\end{enumerate}
\item Classical coarsening (as well as Falgout and CLJP) strongly enforce heuristic 1
\item 
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{PMIS coarsening}
\bit
\item De Sterck, Yang, and Heys. Reducing complexity in parallel algebraic multigrid preconditioners. SIAM J. Mat. Anal. Appl. 2006.
\item Proposed a variation on CLJP that enforces a relaxed version of heuristic 1:
\begin{enumerate}
\item Each F-point must strongly depend on at least one C-point
\end{enumerate}
\item Parallel modified independent set (PMIS) algorithm
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{algorithm}[H]
\caption{CLJP coarsening}
\begin{algorithmic}
\State Select $D = \{i : w(i)>w(j) , \forall j\in S_i\cup S_i^T\}$
\While{ $D\neq \emptyset$ }
\State Mark points in $D$ as C-points
\For{$i\in D$}
   \State Mark points in $S_i^T$ as F-points (communication)
\EndFor
\State Select new $D$ from unmarked points
\EndWhile
\end{algorithmic}
\end{algorithm}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\only<1>{
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/pmisCoarsening1}
\end{center}}
\only<2>{
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/pmisCoarsening2}
\end{center}}
\only<3>{
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/pmisCoarsening3}
\end{center}}
\only<4>{
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/pmisCoarsening4}
\end{center}}
\only<5>{
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/pmisCoarsening5}
\end{center}}
\tiny{Slide credit: Rob Falgout, Copper Mountain 2019 Parallel Multigrid Tutorial}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{PMIS coarsening}
\bit
\item Comparing PMIS to CLJP
\bit
\item Mark C-point neighbors immediately as F-points
\item Initial weights are not updated
\item Satisfies the relaxed heuristic 1, \emph{not} the original
\eit
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{HMIS coarsening}
\bit
\item Similar to Falgout coarsening, can do a hybrid of PMIS and classical coarsening: HMIS
\item Only do first pass of classical coloring algorithm (don't enforce heuristic 1)
\item Again, yields nicer coarsening over processors interiors
\item HMIS is the current default in \emph{hypre}
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/parCoarsening}
\end{center}
\tiny{Figure credit: De Sterck, Yang, and Heys. Reducing complexity in parallel algebraic multigrid preconditioners. SIAM J. Mat. Anal. Appl. 2006.}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{Long-distance interpolation}
\bit
\item PMIS and HMIS can lead to one-sided or no interpolation to some F-points
\item Sparser coarse grids and poor interpolation can yield degraded convergence
\item Need to modify interpolation to better suit sparser coarsenings: long-distance interpolation
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{Long-distance interpolation}
\bit
\item De Sterck, Falgout, Nolting, and Yang. Distance-Two Interpolation for Parallel Algebraic Multigrid. Numer. Lin. Alg. Appl. 2007.
\item Interpolatory sets may include C-points that at distance 2
\item Larger interpolation stencils again increases coarse-grid complexity
\item Can truncate interpolation stencils
\bit
\item Points with one-sided or no interpolation still get the extended stencils
\item Points with large interpolatory sets truncated to reasonable size
\eit
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{Modern best practices AMG}
\bit
\item Overall improvement in time to solution
\eit
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/hypreImprovements1}
\includegraphics[width=0.45\textwidth]{../figures/hypreImprovements2}
\end{center}
\end{block}
\tiny{Figure credit: De Sterck, Falgout, Nolting, and Yang. Distance-Two Interpolation for Parallel Algebraic Multigrid. Numer. Lin. Alg. Appl. 2007.}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{Parallel AMG smoothers}
\bit
\item Baker, Falgout, Kolev, and Yang. Multigrid smoothers for ultra-parallel computing. SIAM J. Sci. Comp. 2011.
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{Parallel AMG takeaways}
\bit
\item Scaling AMG to very large problem sizes on parallel machines is non-trivial
\item Major problem is controlling coarse-grid complexity while retaining good convergence
\item Sparser coarse grids and interpolation stencils lead to lower complexity but can degrade convergence
\item Always trying to optimize accuracy per cost
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{Fine-grained parallelism for AMG}
\bit
\item Naumov, Arsaev, Castonguay, Cohen, Demouth, Eaton, Layton, Markovskiy, Reguly, Sakharnykh, Sellappan, and Strzodka. AMGX: a library for GPU accelerated algebraic multigrid and preconditioned iterative methods.
\item AMGX successfully demonstrated speedup over \emph{hypre}
\item GPU implementation in \emph{hyrpe} is a work in progress
\item Coarsening: CLJP, PMIS
\item Smoothers: modified Jacobi, Chebyshev, colored Gauss-Seidel
\item Everything else is mat-vecs or triple-matrix products
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{More radical approaches}
\bit
\item Ongoing research exploring novel ways to improve AMG's parallel performance:
\bit
\item Sparsification through non-Galerkin coarse grids
\item Additive and asynchronous multigrid methods
\item Algebraic multigrid domain decomposition (AMG-DD)
\eit
\eit
\end{block}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Algebraic multigrid domain decomposition (AMG-DD)}

% Slide 
\begin{frame}{MotivationAlgebraic multigrid domain decomposition (AMG-DD)}
\begin{block}{Communication-limited AMG}
\begin{itemize}
\item Even current best practices can still yield severely communication-limited AMG algorithms due to coarse-grid complexity
\end{itemize}
\end{block}

\begin{center}
\includegraphics[width=0.7\textwidth]{../figures/nonGalerkinFigure}
\end{center}
\tiny{Figure credit: Bienz, Falgout, Gropp, Olson, and Schroder. Reducing parallel communication in algebraic multigrid through sparsification. SIAM J. Sci. Comp. 2016.}

\end{frame}

% Slide 
% This is where AMG-DD comes in.
\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
\begin{block}{Algebraic multigrid domain decomposition (AMG-DD)}
\begin{itemize}
\item Rather than continue modifying components of AMG, design a new algorithm altogether
\item Algebraic multigrid domain decomposition (AMG-DD) utilizes 
\item AMG-DD seeks to reduce communication cost by
\begin{itemize}
\item Enabling more independent computation in between communications
\item Trading moderately more computation for significantly less communication
\end{itemize}
\end{itemize}
\end{block}

\end{frame}

% Slide
\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
\begin{block}{Composite Grids}
\begin{itemize}
\item Each processor solves a representation of the global problem
\item Composite grids resemble adaptively refined meshes but are built using an existing AMG hierarchy
\end{itemize}
\end{block}
\centering
\includegraphics[width=\textwidth]{../figures/compGridCreation1D13}
\end{frame}

% Slide
% Composite grids are constructed as follows. We first choose a value called the padding, for this example we'll use padding 2. Then starting on the fine grid, add all points withing distance 2 through the stencil of the matrix. Then coarsen that set and add those points on the next level to the composite grid. Now simply repeat this process all the way down.
\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
\begin{block}{}
Composite grid creation in 1D with padding 2
\end{block}
\vspace{1 cm}
\only<1>{\includegraphics[width=\textwidth]{../figures/compGridCreation1D3}}
\only<2>{\includegraphics[width=\textwidth]{../figures/compGridCreation1D4}}
\only<3>{\includegraphics[width=\textwidth]{../figures/compGridCreation1D5}}
\only<4>{\includegraphics[width=\textwidth]{../figures/compGridCreation1D6}}
\only<5>{\includegraphics[width=\textwidth]{../figures/compGridCreation1D7}}
\only<6>{\includegraphics[width=\textwidth]{../figures/compGridCreation1D8}}
\only<7>{\includegraphics[width=\textwidth]{../figures/compGridCreation1D9}}
\only<8>{\includegraphics[width=\textwidth]{../figures/compGridCreation1D10}}
\only<9>{\includegraphics[width=\textwidth]{../figures/compGridCreation1D11}}
\only<10>{\includegraphics[width=\textwidth]{../figures/compGridCreation1D12}}
\only<11>{\includegraphics[width=\textwidth]{../figures/compGridCreation1D13}}
\end{frame}

% Slide
\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
\begin{block}{Features of the composite grid}
\begin{itemize}
\item Covers the entire computational domain
\item Cycling requires no communication 
\item Fine resolution in and near the processor subdomain
\item Goal: composite solution is accurate to the full solution over the processor subdomain
\end{itemize}
\end{block}

\end{frame}

% Slide
\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
\begin{block}{AMG-DD pseudo code}
\begin{itemize}
\item Setup:
\begin{itemize}
\item Generate AMG hierarchy
\item Form composite grids
\end{itemize}
\item Iterate:
\begin{itemize}
\item Update fine-grid residual and restrict to all levels (communication)
\item Each processor obtains updated residual at all composite grid points (communication)
\item Each processor cycles on the composite grid (no communication)
\item The global solution is updated by patching together processor subdomains (no communication)
\end{itemize}
\end{itemize}
\end{block}

\end{frame}

%% Slide
%\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
%\begin{block}{Brief details: residual communication}
%\begin{itemize}
%\item Proceeds from coarse to fine level
%\item Communication patterns based on padding
%\end{itemize}
%\end{block}
%
%\centering
%\includegraphics[width=0.7\textwidth]{../figures/resComm}
%
%\hspace{0.5 cm}
%
%\tiny{Figure taken from \emph{Algebraic multigrid domain and range decomposition (AMG-DD/AMG-RD)} by Bank, Falgout, Jones, Manteuffel, McCormick, and Ruge.}
%
%\end{frame}

% Slide
\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
\begin{block}{Brief details: residual communication}
\begin{itemize}
\item Communication on each level moving from coarse to fine
\item Communication stencils are based on padding
\item With padding 1, same communication pattern as operator matrix-vector multiplies on each level 
\end{itemize}
\end{block}

\end{frame}

% Slide
\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
\begin{block}{Brief details: composite-grid solve}
\begin{itemize}
\item Fast adaptive composite (FAC) cycle
\item Action is equivalent to global cycle with suppressed relaxation
\end{itemize}
\end{block}

\centering
\includegraphics[width=\textwidth]{../figures/compGridCreation1D13}

\end{frame}

% Slide
\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
\begin{block}{Test problems}

\begin{align*}
-\nabla \cdot K \nabla u &= \mathbf{1}\,, \hspace{1 cm} \Omega \\
u &= 0\,, \hspace{1 cm} \partial \Omega
\end{align*}

\begin{itemize}
\item 3D Poisson: $K = I$ and $\Omega$ is the unit cube
\item 2D rotated anisotropic diffusion: $\Omega$ is the unit square, $K = Q^TDQ$
\end{itemize}

\begin{align*}
Q &= \begin{bmatrix}
\cos(\theta) & \sin(\theta) \\
-\sin(\theta) & \cos(\theta)
\end{bmatrix}
&
D &= \begin{bmatrix}
1 & 0\\
0 & \epsilon
\end{bmatrix}
\end{align*}
with $\theta=\pi/8$ and $\epsilon=0.001$
\end{block}


\end{frame}

% Slide
\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
\begin{block}{Parameters}
\begin{itemize}
\item Implementation on top of BoomerAMG in hypre
\item AMG best-practices parameters:
\begin{itemize}
   \item Coarsening: HMIS
   \item Interpolation: extended + i
\end{itemize}
\item AMG-DD parameters:
\begin{itemize}
   \item Padding
   \item Number of FAC cycles
\end{itemize}
\item Problem setup:
\begin{itemize}
   \item 256 processors
   \item 12,000 or 18,000 degrees of freedom per processor
\end{itemize}
\end{itemize}
\end{block}

\end{frame}

% Slide
\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
\begin{block}{Padding}
Larger padding yields:
\begin{itemize}
   \item Better accuracy to the global problem
   \item Larger composite grids
   \item More communication and computation
\end{itemize}
\end{block}

\centering
\vspace{0.5 cm}
% \includegraphics[height=0.3\textwidth]{../figures/resConvPadProb0}
% \includegraphics[height=0.3\textwidth]{../figures/resConvPadProb2}

\end{frame}

%% Slide
%\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
%\begin{block}{Padding}
%Larger padding yields:
%\begin{itemize}
%   \item Better accuracy to the global problem
%   \item Larger composite grids
%   \item More communication and computation
%\end{itemize}
%\end{block}
%
%\centering
%\vspace{0.5 cm}
%\includegraphics[height=0.3\textwidth]{../figures/gridSizePadProb0}
%\includegraphics[height=0.3\textwidth]{../figures/gridSizePadProb2}
%
%\end{frame}
%
%% Slide
%\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
%\begin{block}{Padding}
%Larger padding yields:
%\begin{itemize}
%   \item Better accuracy to the global problem
%   \item Larger composite grids
%   \item More communication and computation
%\end{itemize}
%\end{block}
%
%\centering
%\vspace{0.5 cm}
%\includegraphics[height=0.3\textwidth]{../figures/solveNumMessagesByLevel0}
%\includegraphics[height=0.3\textwidth]{../figures/solveNumMessagesByLevel2}
%
%\end{frame}
%
%% Slide
%\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
%\begin{block}{Padding}
%Larger padding yields:
%\begin{itemize}
%   \item Better accuracy to the global problem
%   \item Larger composite grids
%   \item More communication and computation
%\end{itemize}
%\end{block}
%
%\centering
%\vspace{0.5 cm}
%\includegraphics[height=0.3\textwidth]{../figures/solveVolumeByLevel0}
%\includegraphics[height=0.3\textwidth]{../figures/solveVolumeByLevel2}
%
%\end{frame}

% Slide
\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
\begin{block}{Padding}
Accuracy per cost:
\begin{itemize}
   \item Computation
   \item Communication (number of messages and volume)
\end{itemize}
\end{block}

\centering
\vspace{0.5 cm}
% \includegraphics[height=0.3\textwidth]{../figures/resConvByComputation0}
% \includegraphics[height=0.3\textwidth]{../figures/resConvByComputation2}

\end{frame}

% Slide
\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
\begin{block}{Padding}
Accuracy per cost:
\begin{itemize}
   \item Computation
   \item Communication (number of messages and volume)
\end{itemize}
\end{block}

\centering
\vspace{0.5 cm}
% \includegraphics[height=0.3\textwidth]{../figures/resConvByNumMessages0}
% \includegraphics[height=0.3\textwidth]{../figures/resConvByNumMessages2}

\end{frame}

% Slide
\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
\begin{block}{Padding}
Accuracy per cost:
\begin{itemize}
   \item Computation
   \item Communication (number of messages and volume)
\end{itemize}
\end{block}

\centering
\vspace{0.5 cm}
% \includegraphics[height=0.3\textwidth]{../figures/resConvByVolume0}
% \includegraphics[height=0.3\textwidth]{../figures/resConvByVolume2}

\end{frame}

% Slide
\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
\begin{block}{FAC cycles}
More FAC cycles yield:
\begin{itemize}
   \item Better accuracy to the composite solution
   \item More computation
\end{itemize}
\end{block}

\centering
\vspace{0.5 cm}
% \includegraphics[height=0.3\textwidth]{../figures/resConvFACProb0}
% \includegraphics[height=0.3\textwidth]{../figures/resConvFACProb2}

\end{frame}

% Slide
\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
\begin{block}{FAC cycles}
Accuracy per cost:
\begin{itemize}
   \item Computation
   \item More FAC cycles incur no additional communication
\end{itemize}
\end{block}

\centering
\vspace{0.5 cm}
% \includegraphics[height=0.3\textwidth]{../figures/resConvByComputationFAC0}
% \includegraphics[height=0.3\textwidth]{../figures/resConvByComputationFAC2}

\end{frame}

% Slide
\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
%\begin{block}{Additional considerations}
%\begin{itemize}
%\item Recall that communication is most expensive on coarser grids
%\item Finer grids contribute most to storage and computation overhead
%\item Can start AMG-DD on a coarser level:
%\begin{itemize}
%   \item Significantly reduces overhead 
%   \item Retains most of the communication reduction
%\end{itemize}
%\end{itemize}
%\end{block}


\begin{block}{Hybrid AMG-DD}
\begin{itemize}
\item Recall that communication is most expensive on coarser grids
\item Finer grids contribute most to storage and computation overhead
\item Can start AMG-DD on a coarser level:
\begin{itemize}
   \item Significantly reduces overhead 
   \item Retains most of the communication reduction
\end{itemize}
\end{itemize}
\end{block}

\begin{block}{Weak scaling}
\begin{itemize}
\item 42,000 of 30,500 degrees or freedom per processor up to 4,096 processors
\item Padding 1
\end{itemize}
\end{block}

\end{frame}

% Slide
\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
\begin{block}{Computation/storage overhead}
\begin{itemize}
\item Overhead grows slowly with number of processors
\item Use of hybrid AMG-DD can limit overhead
\end{itemize}
\end{block}

\centering
\vspace{0.5 cm}
% \includegraphics[height=0.3\textwidth]{../figures/gridSizeWeakProb0N40000}
% \includegraphics[height=0.3\textwidth]{../figures/gridSizeWeakProb2N10000}

\end{frame}

% Slide
\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
\begin{block}{Number of messages}
\begin{itemize}
\item Number of messages remains small compared to AMG 
\item Hybrid AMG-DD retains most of the reduction in number of messages
\end{itemize}
\end{block}

\centering
\vspace{0.5 cm}
% \includegraphics[height=0.3\textwidth]{../figures/solveNumMessagesWeakProb0N40000}
% \includegraphics[height=0.3\textwidth]{../figures/solveNumMessagesWeakProb2N10000}

\end{frame}

% Slide
\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
\begin{block}{Communication volume}
\begin{itemize}
\item Communication volume grows  slowly with number of processors
\item Hybrid AMG-DD sends almost as much volume as AMG 
\end{itemize}
\end{block}

\centering
\vspace{0.5 cm}
% \includegraphics[height=0.3\textwidth]{../figures/solveVolumeWeakProb0N40000}
% \includegraphics[height=0.3\textwidth]{../figures/solveVolumeWeakProb2N10000}

\end{frame}

%% Slide
%\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
%\begin{block}{Computing regime}
%\begin{itemize}
%\item AMG-DD has been tested up to 4,096 CPUs
%\item Regime is compute-limited
%\end{itemize}
%\end{block}
%
%\centering
%\vspace{0.5 cm}
%\includegraphics[height=0.3\textwidth]{../figures/AspmvTimeByLevelProb0N40000}
%\includegraphics[height=0.3\textwidth]{../figures/AspmvTimeByLevelProb2N10000}
%
%\end{frame}
%
%% Slide
%\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
%\begin{block}{Computing regime}
%\begin{itemize}
%\item GPU clusters present a more communication-limited environment
%\item AMG-DD can provide speedup even for small number of GPUs
%\item 4 GPUs with 250,000 degrees of freedom per GPU
%\end{itemize}
%\end{block}
%
%\centering
%\vspace{0.5 cm}
%\includegraphics[height=0.3\textwidth]{../figures/AspmvTimeByLevel_gpu_P4}
%
%\end{frame}

% Slide
\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
\begin{block}{Timing runs on CPUs}
\begin{itemize}
\item AMG-DD has been tested up to 4,096 CPUs
\item Regime is compute-limited
\item AMG-DD will not provide speedup in a compute-limited regime
\end{itemize}
\end{block}

\begin{block}{Timing runs on GPUs}
\begin{itemize}
\item GPU clusters present a more communication-limited environment
\item AMG-DD can provide speedup even for small number of GPUs
\end{itemize}
\end{block}

\end{frame}

% Slide
\begin{frame}{Algebraic multigrid domain decomposition (AMG-DD)}
\begin{block}{Computing regime}
\begin{itemize}
\item 4 GPUs with 250,000 degrees of freedom per GPU
   \item Computation is relatively much cheaper
   \item AMG-DD with 2 FAC cycles provides greatest speedup
\end{itemize}
\end{block}

\centering
\vspace{0.5 cm}
% \includegraphics[height=0.3\textwidth]{../figures/resConvByTime_gpu_N100}

\end{frame}


% Slide
\begin{frame}{Conclusions}
\begin{block}{Conclusions}
\begin{itemize}
\item The cost of communication is a significant bottleneck for future scalability of AMG
\item AMG-DD is a promising low-communication alternative to AMG
\item AMG-DD achieves:
\begin{itemize}
\item Small storage and computation overhead compared to AMG
\item Similar or better convergence per iteration compared to AMG
\item Large reduction in both number of messages and communication volume compared to AMG
\item Better accuracy per communication cost than AMG
\item Demonstrated speedup in the right computational regime
\end{itemize}
\end{itemize}
\end{block}
\end{frame}

% Slide
\begin{frame}{Conclusions}
\begin{block}{Future work}
\begin{itemize}
\item Run timing runs on larger GPU clusters
\item Use of stronger smoothers such as tridiagonal solvers
\item Use of mixed precision
\item Communication hiding and asynchronous AMG-DD
\item AMG-DD for non-symmetric problems
\item Application of a DD-like algorithm to multigrid reduction in time (MGRIT)
\end{itemize}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

