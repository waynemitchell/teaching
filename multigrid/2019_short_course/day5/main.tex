\documentclass[18pt,xcolor=table]{beamer}

\input{./pres_style.tex}

%%%%%%%%%%%%%%%%%%%%%%%
% user-defined commands
%%%%%%%%%%%%%%%%%%%%%%%
\input{./macros.tex}%added macro definitions here

\usepackage{tikz}
\usepackage{tabularx}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{arrows,positioning} 

\usepackage{cancel}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[]{algorithm}
\usepackage{algpseudocode}
\captionsetup{compatibility=false}


\title[Multigrid]{Introduction to Multigrid Methods}
\subtitle{Day 5: parallel AMG}
\author[Mitchell]{Wayne Mitchell}
\institute{\pgfuseimage{logo}\\Universit\"at Heidelberg\\Institut f\"ur Technische Informatik}
\date[]{\alert{}}


\begin{document}
\input{./slide_style.tex}

\DeclareRobustCommand{\Chi}{\raisebox{2pt}{$\chi$}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Slide
\begin{frame}{}
\begin{block}{Day 5 Goals}
\bit
\item Session 1:
\bit
\item Development of \emph{hypre} BoomerAMG
\eit
\item Session 2:
\bit
\item Algebraic multigrid domain decomposition (AMG-DD)
\eit
\item Session 3:
\bit
\item Discussion and hands-on examples
\eit
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}
\frametitle{\bf Outline:}
\framesubtitle{~~}
\tableofcontents
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Development of \emph{hypre}}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{Parallel AMG}
\bit
\item Recall that the appeal of multigrid is its $O(N)$ computational cost
\item Want to use multigrid for \emph{very} large problems on large parallel machines
\item Efficient parallelization of multigrid, particularly AMG, is non-trivial
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{Parallel AMG}
\bit
\item Much of AMG parallelizes naturally: mostly matrix-vector multiplication
\item AMG setup: need parallel coarsening algorithms and corresponding interpolation strategies
\item AMG solve: need parallel smoothers
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{AMG coarsening in parallel}
\bit
\item Henson and Yang. BoomerAMG: A parallel algebraic multigrid solver and preconditioner. Appl. Num. Math. 2002.
\item Classical coloring algorithm is sequential in nature
\item First attempts at parallel coarsening:
\bit
\item Cleary-Luby-Jones-Plassman (CLJP) coarsening
\item Falgout coarsening
\eit
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{CLJP coarsening}
\bit
\item Based on parallel graph partitioning
\item Entirely parallel, same coarse grids independent of number of processors
\item Admits fine-grained parallelism
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{CLJP coarsening}
\bit
\item Recall strength of connection (SoC): $i$ strongly depends on $j$ if $-a_{i,j} \geq \theta\max_{k\neq i}(-a_{i,k})$
\item Define the SoC matrix, $S$, by $s_{i,j} = 1$ if $i$ strongly depends on $j$
\item Rows, $S_i$, of $S$ define the set of points that strongly influence $i$
\item Columns, $S_i^T$, of $S$ define the set of points that $i$ strongly influences
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{CLJP coarsening}
\bit
\item Define a measure $w(i) = |S_i^T| + \sigma(i)$, where $\sigma(i)$ is a random number in $(0,1)$
\item Select a set $D$, where $i\in D$ if $w(i)>w(j)$ for all $j\in S_i\cup S_i^T$
\item Note that $D$ is an independent set that can be selected in parallel
\item Consider the following heuristics:
\bit
\item C-points are not interpolated from neighbors, so neighbors are less valuable as potential C-points
\item If C-point $i$ influences points $j$ and $k$, and $j$ influences $k$, then $j$ is less valuable as a C-point
\eit
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{algorithm}[H]
\caption{CLJP coarsening}
\begin{algorithmic}
\State Select $D = \{i : w(i)>w(j) , \forall j\in S_i\cup S_i^T\}$
\While{ $D\neq \emptyset$ }
\State Mark points in $D$ as C-points
\For{$i\in D$}
   \State For each $j\in S_i$, decrement $w(j)$ and remove edge $S_{i,j}$
   \For{$j\in S_i^T$}
      \State Remove edge $S_{j,i}$
      \For{$k\in S_j^T : S_{k,i}\neq 0$}
         \State Decrement $w(j)$ and remove edge $S_{k,j}$
      \EndFor
   \EndFor
\EndFor
\State Select new $D$ (communicate weights at processor boundaries)
\EndWhile
\end{algorithmic}
\end{algorithm}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\only<1>{
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/cljpCoarsening1}
\end{center}}
\only<2>{
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/cljpCoarsening2}
\end{center}}
\only<3>{
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/cljpCoarsening3}
\end{center}}
\only<4>{
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/cljpCoarsening4}
\end{center}}
\only<5>{
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/cljpCoarsening5}
\end{center}}
\only<6>{
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/cljpCoarsening6}
\end{center}}
\only<7>{
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/cljpCoarsening7}
\end{center}}
\tiny{Slide credit: Rob Falgout, Copper Mountain 2019 Parallel Multigrid Tutorial}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{CLJP coarsening}
\bit
\item Pros and cons of CLJP coarsening:
\bit
\item Entirely parallel
\item No issues proceeding to arbitrarily coarse grids (some processor drop out)
\item Tends to produce more C-points than classical coarsening 
\item Higher computation and communication complexity on coarse grids
\eit
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{Falgout coarsening}
\bit
\item A more natural extension of classical AMG coarsening in parallel is simply to allow each processor to independently run the Ruge-St\"uben coloring algorithm
\item This yields conflicts at processor boundaries that need to be resolved
\item Several strategies, for this, but the most successful method is to combine Ruge-St\"uben coarsening and CLJP coarsening
\item Result is Falgout coarsening
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{Falgout coarsening}
\bit
\item Start by running both passes of Ruge-St\"uben coloring algorithm independently on each processor
\item User resulting \emph{interior} C-points as initial set, $D$
\item Run CLJP to finish assigning boundary points
\item Results in much nicer coarsenings on processor interiors 
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{PMIS coarsening}
\bit
\item Especially for 3D problems coarse-grid complexity is an issue for Falgout coarsening with classical interpolation
\item Recall the heuristics used for classical AMG coarsening:
\begin{enumerate}
\item For each F-point, $i$, every point $j\in S_i$ is either in $C_i$ or strongly depends on at least one point in $C_i$
\item The set of C-points is a maximal subset such that no C-point strongly depends on another C-point.
\end{enumerate}
\item Classical coarsening (as well as Falgout and CLJP) strongly enforce heuristic 1
\item What if we relax this enforcement?
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{PMIS coarsening}
\bit
\item De Sterck, Yang, and Heys. Reducing complexity in parallel algebraic multigrid preconditioners. SIAM J. Mat. Anal. Appl. 2006.
\item Proposed a variation on CLJP that enforces a relaxed version of heuristic 1:
\begin{enumerate}
\item Each F-point must strongly depend on at least one C-point
\end{enumerate}
\item Parallel modified independent set (PMIS) algorithm
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{algorithm}[H]
\caption{PMIS coarsening}
\begin{algorithmic}
\State Select $D = \{i : w(i)>w(j) , \forall j\in S_i\cup S_i^T\}$
\While{ $D\neq \emptyset$ }
\State Mark points in $D$ as C-points
\For{$i\in D$}
   \State Mark points in $S_i^T$ as F-points (communication)
\EndFor
\State Select new $D$ from unmarked points
\EndWhile
\end{algorithmic}
\end{algorithm}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\only<1>{
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/pmisCoarsening1}
\end{center}}
\only<2>{
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/pmisCoarsening2}
\end{center}}
\only<3>{
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/pmisCoarsening3}
\end{center}}
\only<4>{
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/pmisCoarsening4}
\end{center}}
\only<5>{
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/pmisCoarsening5}
\end{center}}
\tiny{Slide credit: Rob Falgout, Copper Mountain 2019 Parallel Multigrid Tutorial}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{PMIS coarsening}
\bit
\item Comparing PMIS to CLJP
\bit
\item Mark C-point neighbors immediately as F-points
\item Initial weights are not updated
\item Satisfies the relaxed heuristic 1, \emph{not} the original
\eit
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{HMIS coarsening}
\bit
\item Similar to Falgout coarsening, can do a hybrid of PMIS and classical coarsening: HMIS
\item Only do first pass of classical coloring algorithm (don't enforce heuristic 1)
\item Again, yields nicer coarsening over processors interiors
\item HMIS is the current default in \emph{hypre}
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/parCoarsening}
\end{center}
\tiny{Figure credit: De Sterck, Yang, and Heys. Reducing complexity in parallel algebraic multigrid preconditioners. SIAM J. Mat. Anal. Appl. 2006.}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{Long-distance interpolation}
\bit
\item PMIS and HMIS can lead to one-sided or no interpolation to some F-points
\item Sparser coarse grids and poor interpolation can yield degraded convergence
\item Need to modify interpolation to better suit sparser coarsenings: long-distance interpolation
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{Long-distance interpolation}
\bit
\item De Sterck, Falgout, Nolting, and Yang. Distance-Two Interpolation for Parallel Algebraic Multigrid. Numer. Lin. Alg. Appl. 2007.
\item Interpolatory sets may include C-points that at distance 2
\item Larger interpolation stencils again increases coarse-grid complexity
\item Can truncate interpolation stencils
\bit
\item Points with one-sided or no interpolation still get the extended stencils
\item Points with large interpolatory sets truncated to reasonable size
\eit
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{Modern best practices AMG}
\bit
\item Overall improvement in time to solution
\bit
\item Unstructured 3D diffusion with discontinuous coefficients
\item Unstructured 3D elasticity 
\eit
\eit
\begin{center}
\includegraphics[width=0.5\textwidth]{../figures/hypreImprovements1}
\includegraphics[width=0.45\textwidth]{../figures/hypreImprovements2}
\end{center}
\end{block}
\tiny{Figure credit: De Sterck, Falgout, Nolting, and Yang. Distance-Two Interpolation for Parallel Algebraic Multigrid. Numer. Lin. Alg. Appl. 2007.}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{Parallel AMG smoothers}
\bit
\item Baker, Falgout, Kolev, and Yang. Multigrid smoothers for ultra-parallel computing. SIAM J. Sci. Comp. 2011.
\item Gauss-Seidel is the classical method of choice... does not parallelize!
\item Simple hybrid Gauss-Seidel does not have scalable convergence as processor count grows
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{Parallel AMG smoothers}
\bit
\item Weighted Jacobi: need to determine problem-dependent optimal weighting parameter
\item Colored Gauss-Seidel: can have many colors in practice, especially on coarse grids
\item C-F smoothers: use the C/F splitting as a coloring for Jacobi-like smoothers
\item Polynomial smoothers: application only involves mat-vec, but requires eigenvalue estimates
\item $l_1$-weighted smoothers: restore robustness for Jacobi and hybrid Gauss-Seidel
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{Parallel AMG takeaways}
\bit
\item Scaling AMG to very large problem sizes on parallel machines is non-trivial
\item Smoothers need to be not only parallel but also convergent independent of the parallelism
\item Major problem is controlling coarse-grid complexity while retaining good convergence
\item Sparser coarse grids and interpolation stencils lead to lower complexity but can degrade convergence
\item Always trying to optimize accuracy per cost
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{Fine-grained parallelism for AMG}
\bit
\item Naumov, Arsaev, Castonguay, Cohen, Demouth, Eaton, Layton, Markovskiy, Reguly, Sakharnykh, Sellappan, and Strzodka. AMGX: a library for GPU accelerated algebraic multigrid and preconditioned iterative methods.
\item AMGX successfully demonstrated speedup over \emph{hypre}
\item GPU implementation in \emph{hypre} is a work in progress
\item Coarsening: CLJP, PMIS
\item Smoothers: C/F $l_1$ Jacobi, Chebyshev
\item Everything else is mat-vecs or triple-matrix products
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{Development of \emph{hypre}}
\begin{block}{More radical approaches}
\bit
\item Ongoing research exploring novel ways to improve AMG's parallel performance:
\bit
\item Sparsification through non-Galerkin coarse grids
\item Additive and asynchronous multigrid methods
\item Algebraic multigrid domain decomposition (AMG-DD)
\eit
\eit
\end{block}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Algebraic multigrid domain decomposition (AMG-DD)}

% Slide 
\begin{frame}{AMG-DD}
\begin{block}{Communication-limited AMG}
\bit
\item Current best practices can still yield severely communication-limited AMG algorithms
\eit
\end{block}
\begin{center}
\includegraphics[width=0.7\textwidth]{../figures/nonGalerkinFigure}
\end{center}
\tiny{Figure credit: Bienz, Falgout, Gropp, Olson, and Schroder. Reducing parallel communication in algebraic multigrid through sparsification. SIAM J. Sci. Comp. 2016.}
\end{frame}

% Slide 
% This is where AMG-DD comes in.
\begin{frame}{AMG-DD}
\begin{block}{Algebraic multigrid domain decomposition (AMG-DD)}
\bit
\item Rather than continue modifying components of AMG, design a new algorithm altogether
\item Algebraic multigrid domain decomposition (AMG-DD) builds on top of an existing AMG hierarchy
\item AMG-DD seeks to reduce communication cost by
\bit
\item Enabling more independent computation in between communications
\item Trading moderately more computation for significantly less communication
\eit
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{Composite Grids}
\bit
\item Standard parallelization of AMG: each processor owns a subdomain on the fine grid and all coarse-grid points underneath
\eit
\end{block}
\centering
\includegraphics[width=\textwidth]{../figures/compGridCreation1D2}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{Composite Grids}
\bit
\item Each processor stores a composite representation of the global problem
\item Composite grids resemble adaptively refined meshes but are built using an existing AMG hierarchy
\eit
\end{block}
\centering
\includegraphics[width=\textwidth]{../figures/compGridCreation1D13}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{}
Composite grid creation in 1D with padding 2
\end{block}
\vspace{1 cm}
\only<1>{\includegraphics[width=\textwidth]{../figures/compGridCreation1D3}}
\only<2>{\includegraphics[width=\textwidth]{../figures/compGridCreation1D4}}
\only<3>{\includegraphics[width=\textwidth]{../figures/compGridCreation1D5}}
\only<4>{\includegraphics[width=\textwidth]{../figures/compGridCreation1D6}}
\only<5>{\includegraphics[width=\textwidth]{../figures/compGridCreation1D7}}
\only<6>{\includegraphics[width=\textwidth]{../figures/compGridCreation1D8}}
\only<7>{\includegraphics[width=\textwidth]{../figures/compGridCreation1D9}}
\only<8>{\includegraphics[width=\textwidth]{../figures/compGridCreation1D10}}
\only<9>{\includegraphics[width=\textwidth]{../figures/compGridCreation1D11}}
\only<10>{\includegraphics[width=\textwidth]{../figures/compGridCreation1D12}}
\only<11>{\includegraphics[width=\textwidth]{../figures/compGridCreation1D13}}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{Features of the composite grid}
\bit
\item Covers the entire computational domain
\item Cycling requires no communication 
\item Fine resolution in and near the processor subdomain
\item Goal: composite solution is accurate to the full solution over the processor subdomain
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{AMG-DD pseudo code}
\bit
\item Setup:
\bit
\item Generate AMG hierarchy
\item Form composite grids
\eit
\item Iterate:
\bit
\item Update fine-grid residual and restrict to all levels (communication)
\item Each processor obtains updated residual at all composite grid points (communication)
\item Each processor cycles on the composite grid (no communication)
\item The global solution is updated by patching together processor subdomains (no communication)
\eit
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{Composite grid residual communication}
\bit
\item Proceeds from coarse to fine level
\item Communication patterns based on padding
\item Communicated data is halo data plus a corresponding sub-composite grid
\eit
\end{block}
\begin{center}
\includegraphics[width=0.7\textwidth]{../figures/resComm}
\end{center}
\tiny{Figure credit: Bank, Falgout, Jones, Manteuffel, McCormick, and Ruge. Algebraic multigrid domain and range decomposition (AMG-DD/AMG-RD). SIAM J. Sci. Comp. 2015.}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{Composite grid residual communication}
\bit
\item Communicating the full $\Psi \cup \Psi_c$ sets sends some redundant data
\item Redundancy can be avoided in implementation
\item Communication stencils determined by padding
\item With padding 1, same communication pattern as mat-vec
\item Communicate halo data (with depth equal to the padding) plus some coarser points
\item With padding 1, slightly more data communicated vs. mat-vec
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{Composite-grid solve: AlgFAC cycles}
\bit
\item Fast adaptive composite (FAC) cycle were developed for geometric multigrid with variable grid refinement
\item Action is equivalent to cyclingd with suppressed relaxation
\eit
\end{block}
\centering
\includegraphics[width=\textwidth]{../figures/compGridCreation1D13}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{AlgFAC cycle relaxation}
\bit
\item Relaxation is simple
\item Need one layer of "ghost" points
\eit
\end{block}
\centering
\includegraphics[width=0.8\textwidth]{../figures/D}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{AlgFAC cycle interpolation}
\bit
\item Interpolation requires some assumptions
\item Need correct interpolation to real points plus the extra ghost layer required for relaxation
\eit
\end{block}
\centering
\includegraphics[width=0.8\textwidth]{../figures/PD}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{AlgFAC cycle restriction}
\bit
\item Restriction is complicated
\item Can't do standard inter-grid transfer for restriction
\eit
\end{block}
\centering
\includegraphics[width=0.8\textwidth]{../figures/restrictionStencils}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{AlgFAC cycle restriction}
\bit
\item Observe that away from fine-grid relaxation, can replace restriction with residual recalculation
\item Can rewrite restriction such that only the affect of relaxation is transfered between grids
\eit
\end{block}
\centering
\includegraphics[width=0.8\textwidth]{../figures/restrictionStencils}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{AlgFAC cycle}
\bit
\item In the end, AlgFAC requires only one layer of ghost points
\item Need only solution values (no right-hand side) at ghost points
\item No need to communicate any info at ghost points
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{Test problems}
\begin{align*}
-\nabla \cdot K \nabla u &= \mathbf{1}\,, \hspace{1 cm} \Omega \\
u &= 0\,, \hspace{1 cm} \partial \Omega
\end{align*}
\bit
\item 3D Poisson: $K = I$ and $\Omega$ is the unit cube
\item 2D rotated anisotropic diffusion: $\Omega$ is the unit square, $K = Q^TDQ$
\eit
\begin{align*}
Q &= \begin{bmatrix}
\cos(\theta) & \sin(\theta) \\
-\sin(\theta) & \cos(\theta)
\end{bmatrix}
&
D &= \begin{bmatrix}
1 & 0\\
0 & \epsilon
\end{bmatrix}
\end{align*}
with $\theta=\pi/8$ and $\epsilon=0.001$
\end{block}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{Parameters}
\bit
\item Implementation on top of BoomerAMG in hypre
\item AMG parameters:
\bit
   \item Coarsening: HMIS
   \item Interpolation: extended + i
   \item L1 CF Jacobi smoothing
\eit
\item AMG-DD parameters:
\bit
   \item Padding
   \item Number of FAC cycles
\eit
\item Problem setup:
\bit
   \item 256 processors
   \item 100,000 degrees of freedom per processor
\eit
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{Padding}
Larger padding yields:
\bit
   \item Better accuracy to the global problem
   \item Larger composite grids
   \item More communication and computation
\eit
\end{block}
\centering
\vspace{0.5 cm}
\includegraphics[height=0.3\textwidth]{../figures/resConvPadByIterationProblem1}
\includegraphics[height=0.3\textwidth]{../figures/resConvPadByIterationProblem2}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{Padding}
Larger padding yields:
\bit
  \item Better accuracy to the global problem
  \item Larger composite grids
  \item More communication and computation
\eit
\end{block}
\centering
\vspace{0.5 cm}
\includegraphics[height=0.3\textwidth]{../figures/gridSizePadProb0}
\includegraphics[height=0.3\textwidth]{../figures/gridSizePadProb2}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{Padding}
Larger padding yields:
\bit
  \item Better accuracy to the global problem
  \item Larger composite grids
  \item More communication and computation
\eit
\end{block}
\centering
\vspace{0.5 cm}
\includegraphics[height=0.3\textwidth]{../figures/solveVolumePadByLevelProblem1}
\includegraphics[height=0.3\textwidth]{../figures/solveVolumePadByLevelProblem2}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{Padding}
Larger padding yields:
\bit
  \item Better accuracy to the global problem
  \item Larger composite grids
  \item More communication and computation
\eit
\end{block}

\centering
\vspace{0.5 cm}
\includegraphics[height=0.3\textwidth]{../figures/solveNumMessagesPadByLevelProblem1}
\includegraphics[height=0.3\textwidth]{../figures/solveNumMessagesPadByLevelProblem2}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{Padding}
Accuracy per cost:
\bit
   \item Computation
   \item Communication (number of messages and volume)
\eit
\end{block}
\centering
\vspace{0.5 cm}
\only<1>{
\includegraphics[height=0.3\textwidth]{../figures/resConvPadByComputationProblem1}
\includegraphics[height=0.3\textwidth]{../figures/resConvPadByComputationProblem2}
}
\only<2>{
\includegraphics[height=0.3\textwidth]{../figures/resConvPadByLatencyProblem1}
\includegraphics[height=0.3\textwidth]{../figures/resConvPadByLatencyProblem2}
}
\only<3>{
\includegraphics[height=0.3\textwidth]{../figures/resConvPadByVolumeProblem1}
\includegraphics[height=0.3\textwidth]{../figures/resConvPadByVolumeProblem2}
}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{AlgFAC cycles}
More AlgFAC cycles yield:
\bit
   \item Better accuracy to the composite solution
   \item More computation
\eit
\end{block}
\centering
\vspace{0.5 cm}
\includegraphics[height=0.3\textwidth]{../figures/resConvNumFACByIterationProblem1}
\includegraphics[height=0.3\textwidth]{../figures/resConvNumFACByIterationProblem2}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{AlgFAC cycles}
Accuracy per cost:
\bit
   \item Computation
   \item More AlgFAC cycles incur no additional communication
\eit
\end{block}
\centering
\vspace{0.5 cm}
\includegraphics[height=0.3\textwidth]{../figures/resConvNumFACByComputationProblem1}
\includegraphics[height=0.3\textwidth]{../figures/resConvNumFACByComputationProblem2}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{Weak scaling results}
\bit
\item Weak scaling results for 16 up to 1024 processors
\item Run on Piz Daint on GPU accelerated nodes
\item 250,000 degrees of freedom per processor
\eit
\end{block}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{Composite-grid overhead}
\bit
\item Manageable growth of composite-grid overhead
\item Measured by relative number of matrix non-zeros
\eit
\end{block}
\centering
\vspace{0.5 cm}
\includegraphics[height=0.3\textwidth]{../figures/gridSizeProblem1}
\includegraphics[height=0.3\textwidth]{../figures/gridSizeProblem2}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{Communication reduction}
\bit
\item Significant reduction in latency and bandwidth cost
\item Steady as processor count grows
\eit
\end{block}
\centering
\vspace{0.5 cm}
\includegraphics[height=0.3\textwidth]{../figures/solveCommCostProblem1}
\includegraphics[height=0.3\textwidth]{../figures/solveCommCostProblem2}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{Solve time}
\bit
\item Reduction in per-iteration solve time
\item Faster despite doing more than twice the computation
\eit
\end{block}
\centering
\vspace{0.5 cm}
\includegraphics[height=0.3\textwidth]{../figures/solveTimeProblem1}
\includegraphics[height=0.3\textwidth]{../figures/solveTimeProblem2}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{Accuracy per time}
\bit
\item Similar or better per iteration convergence 
\item Effective convergence factor scaled by run time 
\eit
\end{block}
\centering
\vspace{0.5 cm}
\includegraphics[height=0.3\textwidth]{../figures/resConvFactProblem1}
\includegraphics[height=0.3\textwidth]{../figures/resConvFactProblem2}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{Decreased time to solution}
\bit
\item Clear benefit at large processor counts in accuracy per time
\eit
\end{block}
\centering
\vspace{0.5 cm}
\includegraphics[height=0.3\textwidth]{../figures/resConvByTimeProblem1}
\includegraphics[height=0.3\textwidth]{../figures/resConvByTimeProblem2}
\end{frame}

% Slide
\begin{frame}{AMG-DD}
\begin{block}{Future work}
\bit
\item AMG-DD performs smoothing independently on each processor
\bit
\item Stronger smoothers (block solves, ILU, etc.) are normally communication-limited
\item AMG-DD could enable much more efficient use of such smoothers
\eit
\item Develop corresponding coarsening/interpolation strategies 
\item Use of mixed precision
\eit
\end{block}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

